{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8b5290d-a837-42c2-8496-49467685647c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data preparation for the RAG tool that will serve information to the model\n",
    "\n",
    "Developer: Eliel Paes    \n",
    "Last update: 2025-12-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a5754cf-d679-4c0e-ac78-c171da81124d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8886666f-97db-44c9-83d0-8d21a037171c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt \"unstructured[local-inference]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "482c5a6d-f687-4272-af36-f54a13fa42a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af5a190e-65e9-4a29-a39a-91d869878423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import fitz\n",
    "\n",
    "from llama_index.core.langchain_helpers.text_splitter import SentenceSplitter\n",
    "from llama_index.core import Document, set_global_tokenizer\n",
    "from typing import Iterator\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from unstructured.partition.auto import partition\n",
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "client = get_deploy_client(\"databricks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "296dbc77-41d5-487d-97b7-a4c4654823cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1 Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c17bbf2-62b7-417d-bced-7fc60b6e9554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.1 Extracting pdf raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f06c72c-6e72-4314-a5d9-9b4966ff4ff4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining some variables\n",
    "articles_path = \"/Volumes/workspace/default/study_files\"\n",
    "catalog = \"workspace\"\n",
    "db_name = \"default\"\n",
    "table_name = f\"pdf_raw_text\"\n",
    "\n",
    "# reading pdf files as binary\n",
    "# df schema will be: \n",
    "      # |-- path: string (nullable = true)\n",
    "      # |-- modificationTime: timestamp (nullable = true)\n",
    "      # |-- length: long (nullable = true)\n",
    "      # |-- content: binary (nullable = true)      \n",
    "df = (spark.read.format(\"binaryFile\")\n",
    "      .option(\"recursiveFileLookup\", \"true\")\n",
    "      .load(articles_path))\n",
    "\n",
    "# save data to a deltatable\n",
    "# df.write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc4bf04a-647d-4896-8c2b-fe633d881c4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Functions to be apllied in the content column in order to extract the pad raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9de69919-ca86-4295-b542-58da622cd6fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf(\"array<string>\")\n",
    "def get_pdf_raw_text(contents: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"\n",
    "        Text extraction from pdf files. \n",
    "\n",
    "        The content of pdf files are in the content column of our pyspark dataframe, so we will iterate over the content column and extract the text from each pdf file.    \n",
    "    \"\"\"     \n",
    "    def extract_doc_text(col):\n",
    "        \"\"\"\n",
    "            receives e pdf (bytes) and return the extracted text.\n",
    "        \n",
    "        \"\"\"    \n",
    "        content = []\n",
    "        try:\n",
    "            pdf = fitz.open(stream = col)\n",
    "            for page in pdf.pages():\n",
    "                content.append(str(json.dumps({\"page\": page.number + 1, \"content\": page.get_text()})))\n",
    "\n",
    "            return content       \n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao carregar arquivo {b}: {e}\")\n",
    "\n",
    "    for batch in contents: \n",
    "        yield batch.apply(extract_doc_text)    \n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def get_article_topics(pdf_pages: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"        \n",
    "    def get_topics(col):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        print(col)\n",
    "        text = json.loads(col)\n",
    "        return text[\"content\"]\n",
    "\n",
    "    for page in pdf_pages:\n",
    "        yield page.apply(get_topics)\n",
    "\n",
    "\n",
    "@pandas_udf(\"int\")\n",
    "def get_page_number(pdf_pages: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"        \n",
    "    def get_topics(col):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        print(col)\n",
    "        text = json.loads(col)\n",
    "        return text[\"page\"]\n",
    "    \n",
    "    for page in pdf_pages:\n",
    "        yield page.apply(get_topics)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e7327a9-71a7-43a2-b35e-470b660ecd8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_raw_text = df.select(\"path\", \"content\")\n",
    "\n",
    "df_raw_text = df_raw_text.withColumn(\"pdf_raw_text\", get_pdf_raw_text(F.col(\"content\"))).drop(\"content\")\n",
    " \n",
    "df_raw_text = df_raw_text.withColumn(\"pdf_raw_text\", F.explode(F.col(\"pdf_raw_text\")))\n",
    "\n",
    "df_raw_text = df_raw_text.withColumn(\"pdf_text_page_content\", get_article_topics(F.col(\"pdf_raw_text\")))\n",
    "\n",
    "df_raw_text = df_raw_text.withColumn(\"page\", get_page_number(F.col(\"pdf_raw_text\"))) \n",
    " \n",
    "df_raw_text.select(\"path\", \"page\", \"pdf_text_page_content\").write.mode(\"overwrite\").saveAsTable(f\"{catalog}.{db_name}.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b08b1a5-7c32-48ae-a263-65cf67f8f760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### 1.2 Extracting pdf images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5de81ff-3875-44be-b035-1ec76a161ac7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Functions to be apllied in the content column in order to extract the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0350a621-5887-4163-9f8b-0790abcd2222",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_images_from_pdf(content):\n",
    "    \"\"\"\n",
    "    Extracts images from a PDF file using PyMuPDF and saves them to a folder.\n",
    "    \"\"\"\n",
    "    # 1. Open the PDF\n",
    "    try:\n",
    "        doc = fitz.open(stream = content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening PDF: {e}\")\n",
    "        return\n",
    "\n",
    "    image_count = 0\n",
    "    \n",
    "    # Iterate through all pages\n",
    "    for page in doc.pages():\n",
    "        \n",
    "        # 2. Get a list of image objects on the current page\n",
    "        # 'get_images' returns a list of tuples with image details\n",
    "        image_list = page.get_images(full=True)\n",
    "\n",
    "        # Iterate through the image list\n",
    "        for img_index, img_info in enumerate(image_list):\n",
    "            xref = img_info[0]  # The XREF is the internal reference number of the image object\n",
    "            \n",
    "            # 3. Extract the image data\n",
    "            # get_pixmap is usually used for rendering, but we use extract_image for raw data\n",
    "            image_data = pdf_document.extract_image(xref)\n",
    "            \n",
    "            # 4. Get the image properties\n",
    "            image_bytes = image_data[\"image\"]\n",
    "            image_ext = image_data[\"ext\"]\n",
    "            \n",
    "            # 5. Determine the filename\n",
    "            filename = f\"page{page_index+1}_img{img_index+1}_{xref}.{image_ext}\"\n",
    "            image_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            # 6. Save the image file\n",
    "            try:\n",
    "                # Use standard file writing for raw image formats (PNG, JPEG)\n",
    "                if image_ext in [\"png\", \"jpg\", \"jpeg\", \"bmp\"]:\n",
    "                     with open(image_path, \"wb\") as f:\n",
    "                        f.write(image_bytes)\n",
    "                else:\n",
    "                    # For other formats (like TIFF or masked images), use PIL to handle them\n",
    "                    image_stream = io.BytesIO(image_bytes)\n",
    "                    img = Image.open(image_stream)\n",
    "                    # Convert to RGB to ensure compatibility and save as PNG\n",
    "                    img.convert(\"RGB\").save(image_path + \".png\")\n",
    "                    filename = filename + \".png\" # Update filename for logging\n",
    "                \n",
    "                print(f\"Successfully extracted: {filename}\")\n",
    "                image_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to save image {filename}. Error: {e}\")\n",
    "\n",
    "    pdf_document.close()\n",
    "    print(f\"\\n--- Extraction Complete. Total images extracted: {image_count} ---\")\n",
    "\n",
    "\n",
    "# --- Usage Example ---\n",
    "# Replace 'your_file.pdf' with the actual path to your PDF file\n",
    "pdf_file = \"your_file.pdf\"\n",
    "extract_images_from_pdf(pdf_file)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_preparation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
